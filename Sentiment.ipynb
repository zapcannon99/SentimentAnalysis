{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "Author: Joel Yin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Tenkichi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Tenkichi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Library Imports\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import wordnet as wn\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "import re\n",
    "import copy\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "def load_corpus(corpus_path):\n",
    "    file = open(corpus_path, encoding=\"ISO-8859-1\", mode=\"r\")\n",
    "    lines = file.readlines()\n",
    "    dataset = list()\n",
    "    for line in lines:\n",
    "        snippet, label = line.split(\"\\t\")\n",
    "        label = int(label.strip())\n",
    "        dataset.append(tuple([snippet, label]))\n",
    "    return dataset\n",
    "\n",
    "re_lrquotes = r\"^'(.+)'$\"\n",
    "re_lquotes = r\"^'(.+)$\"\n",
    "re_rquotes = r\"^(.+)'$\"\n",
    "\n",
    "def tokenize(snippet):\n",
    "    blurb = copy.deepcopy(snippet)\n",
    "    blurb = re.sub(re_lrquotes, r\"' \\1 '\", blurb)\n",
    "    blurb = re.sub(re_lquotes, r\"' \\1\", blurb)\n",
    "    blurb = re.sub(re_rquotes, r\"\\1 '\", blurb)\n",
    "    return blurb.split()\n",
    "        \n",
    "def tag_edits(tokenized_snippet):\n",
    "    tokens = list()\n",
    "    edited = False;\n",
    "    for token in tokenized_snippet:\n",
    "        temp = token\n",
    "        if not edited:\n",
    "            if temp.find(\"[\") > -1:\n",
    "                edited = True\n",
    "                \n",
    "        if edited:\n",
    "            temp = re.sub(r\"\\[?(\\w+)\", r\"EDIT_\\1\", temp)\n",
    "            if temp.find(\"]\") > -1:\n",
    "                temp = re.sub(r\"\\]\", \"\", temp)\n",
    "                edited = False\n",
    "        \n",
    "        tokens.append(temp)\n",
    "                \n",
    "    return tokens\n",
    "                \n",
    "re_negation = r\"^not$|^no$|^never$|^cannot$|^nothing$|^neither$|^nor$|^nobody$|^nowhere$|^except$|n't$\"\n",
    "re_negation_stop = r\"^but$|^yet$|^however$|^nevertheless$|^though$|^except$|^\\.$|^\\?$|^\\!$\"\n",
    "re_negation_quickstops = r\"^only$|^just$\"\n",
    "\n",
    "def tag_negation(tokenized_snippet):\n",
    "    # first remove any meta tags to make it \"pure\" again\n",
    "    tokens = copy.deepcopy(tokenized_snippet)\n",
    "    metas = list()\n",
    "    for i, token in enumerate(tokens):\n",
    "        pattern = r\"(\\w+)_(\\w+)\"\n",
    "        match = re.search(pattern, token)\n",
    "        if match:\n",
    "            metas.append(tuple([i, match.group(1)]))\n",
    "            tokens[i] = match.group(2)\n",
    "            \n",
    "    if \"\" in tokens:\n",
    "        tokens.remove(\"\")\n",
    "    tokens = nltk.pos_tag(tokens)\n",
    "    \n",
    "    for i, edit in metas:\n",
    "        word, pos = tokens[i]\n",
    "        tokens[i] = tuple([edit + \"_\" + word, pos])\n",
    "    # Using a variable that switches states for NOT/not_NOT sections\n",
    "    negated = False\n",
    "    indexes = range(0, len(tokens))\n",
    "    for i in indexes:\n",
    "        word, pos = tokens[i]\n",
    "        if not negated:\n",
    "            if re.search(re_negation, word):\n",
    "                # When we find a not word, we need to check the next word for stop word\n",
    "                if(i+1 < len(tokens)):\n",
    "                    next_word = tokens[i+1][0]\n",
    "                else:\n",
    "                    next_word = \"\"\n",
    "                if re.search(re_negation_quickstops, next_word):\n",
    "                    i = i + 1\n",
    "                else:\n",
    "                    negated = True\n",
    "        else:\n",
    "            if re.search(re_negation_stop, word) or pos==\"JJR\" or pos==\"RBR\":\n",
    "                negated = False\n",
    "                # checking for the negation connection words\n",
    "            elif not re.search(r\"^or$|^nor$\", word):\n",
    "                tokens[i] = tuple([\"NOT_\" + word, pos])\n",
    "                \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(corpus_path):\n",
    "    dataset = load_corpus(corpus_path)\n",
    "    for i, pair in enumerate(dataset):\n",
    "        y = pair[1]\n",
    "        X = pair[0]\n",
    "        X = tag_negation(tag_edits(tokenize(X)))\n",
    "        dataset[i] = tuple([X, y])\n",
    "        \n",
    "    return dataset\n",
    "        \n",
    "\n",
    "trn = preprocess(\"train.txt\")\n",
    "tst = preprocess(\"test.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = dict()\n",
    "dictionary_reverse = dict()\n",
    "\n",
    "for datapoint, label in trn:\n",
    "    for pair in datapoint:\n",
    "        # first check if token has EDIT tag\n",
    "        token = pair[0]\n",
    "        if not re.search(\"EDIT\", token):\n",
    "            if token not in dictionary:\n",
    "                dictionary_reverse[len(dictionary)] = token\n",
    "                dictionary[token] = len(dictionary)\n",
    "dictionary_size = len(dictionary.keys())\n",
    "                \n",
    "def get_features(preprocessed_snippet):\n",
    "    feature_counts = np.zeros(dictionary_size)\n",
    "    for (token, pos) in preprocessed_snippet:\n",
    "        if not re.search(\"EDIT\", token):\n",
    "            if token in dictionary:\n",
    "                feature_index = dictionary[token]\n",
    "                feature_counts[feature_index] = feature_counts[feature_index] + 1\n",
    "    return feature_counts\n",
    "\n",
    "X_train = np.empty((len(trn), dictionary_size))\n",
    "Y_train = np.empty(len(trn)).astype(int)\n",
    "for i, (datapoint, label) in enumerate(trn):\n",
    "    Y_train[i] = label\n",
    "    features = get_features(datapoint)\n",
    "    X_train[i] = features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize\n",
    "def normalize(X):\n",
    "    matrix = copy.deepcopy(X)\n",
    "    feature_indexes = np.arange(X.shape[1])\n",
    "    maxes = np.max(matrix, axis=0)\n",
    "    mins = np.min(matrix, axis=0)\n",
    "    for index in feature_indexes:\n",
    "        column = matrix[:, index]\n",
    "        if maxes[index] - mins[index] == 0:\n",
    "            matrix[:, index] = np.zeros(X.shape[0])\n",
    "        else:\n",
    "            matrix[:, index] = (column - mins[index])/(maxes[index] - mins[index])\n",
    "    return matrix\n",
    "\n",
    "X_train_normalized = normalize(X_train)\n",
    "classifier_Gaussian = GaussianNB()\n",
    "model = classifier_Gaussian.fit(X_train_normalized, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_predictions(Y_pred, Y_true):\n",
    "    tp, tn, fp, fn = np.zeros(4).astype(int)\n",
    "    for index in range(len(Y_pred)):\n",
    "        pred = Y_pred[index]\n",
    "        true = Y_true[index]\n",
    "        if(pred == 1):\n",
    "            if(true == 1):\n",
    "                tp = tp + 1\n",
    "            else:\n",
    "                fp = fp + 1\n",
    "        else:\n",
    "            if(true == 0):\n",
    "                tn = tn + 1\n",
    "            else:\n",
    "                fn = fn + 1\n",
    "\n",
    "    precision = tp/(tp + fp)\n",
    "    recall = tp/(tp + fn)\n",
    "    fmeasure = (2 * precision * recall)/(precision + recall)\n",
    "    return (precision, recall, fmeasure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tst = preprocess(\"test.txt\")\n",
    "\n",
    "# for datapoint, label in tst:\n",
    "#     for pair in datapoint:\n",
    "#         # first check if token has EDIT tag\n",
    "#         token = pair[0]\n",
    "#         if not re.search(\"EDIT\", token):\n",
    "#             if token not in dictionary:\n",
    "#                 dictionary[token] = len(dictionary)\n",
    "\n",
    "# dictionary_size = len(dictionary.keys())\n",
    "\n",
    "X_test = np.empty((len(tst), dictionary_size))\n",
    "Y_true = np.empty(len(tst)).astype(int)\n",
    "for i, (datapoint, label) in enumerate(tst):\n",
    "    Y_true[i] = label\n",
    "    features = get_features(datapoint)\n",
    "    X_test[i] = features\n",
    "    \n",
    "X_test_normalized = normalize(X_test)\n",
    "\n",
    "Y_pred = model.predict(normalize(X_test_normalized))\n",
    "precision, recall, fmeasure = evaluate_predictions(Y_pred, Y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB Scores:\n",
      "precision: 0.6123348017621145\n",
      "recall: 0.8398791540785498\n",
      "fmeasure: 0.7082802547770701\n"
     ]
    }
   ],
   "source": [
    "print(\"GaussianNB Scores:\")\n",
    "print(\"precision: {}\".format(precision))\n",
    "print(\"recall: {}\".format(recall))\n",
    "print(\"fmeasure: {}\".format(fmeasure))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tenkichi\\.julia\\conda\\3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression Scores (pre DAL and WebNet):\n",
      "precision: 0.6822660098522167\n",
      "recall: 0.8368580060422961\n",
      "fmeasure: 0.751696065128901\n"
     ]
    }
   ],
   "source": [
    "classifier_LR = LogisticRegression()\n",
    "model = classifier_LR.fit(X_train_normalized, Y_train)\n",
    "Y_pred = model.predict(X_test)\n",
    "precision, recall, fmeasure = evaluate_predictions(Y_pred, Y_true)\n",
    "print(\"LogisticRegression Scores (pre DAL and WebNet):\")\n",
    "print(\"precision: {}\".format(precision))\n",
    "print(\"recall: {}\".format(recall))\n",
    "print(\"fmeasure: {}\".format(fmeasure))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('too', -3.7107639626498323), ('bad', -2.9583008069901195), ('dull', -2.5381501747452275), ('fails', -2.2214358327354966), ('funny', 2.1107629504287986), ('boring', -1.9831300308600917), ('engrossing', 1.9199729826132301), ('entertaining', 1.8785160899899584), ('worst', -1.8648488923424085), ('worth', 1.8586615698105307)]\n"
     ]
    }
   ],
   "source": [
    "def by_coef(element):\n",
    "    return element[1]\n",
    "\n",
    "def top_features(logreg_model, k):\n",
    "    coefs = np.abs(logreg_model.coef_.flatten())\n",
    "    top_indexes = np.argsort(coefs)[-k::]\n",
    "    top_features = list()\n",
    "    for index in top_indexes:\n",
    "        if index - dictionary_size + 1 == 1:\n",
    "            word = \"<activeness>\"\n",
    "        elif index - dictionary_size + 1 == 2:\n",
    "            word = \"<evaluation>\"\n",
    "        elif index - dictionary_size + 1 == 3:\n",
    "            word = \"<imagery>\"\n",
    "        else:\n",
    "            word = dictionary_reverse[index]\n",
    "            \n",
    "        top_features.append(tuple([word, logreg_model.coef_.flatten()[index]]))\n",
    "    top_features.reverse()\n",
    "    return top_features\n",
    "    \n",
    "print(top_features(model, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dal(dal_path):\n",
    "    file = open(dal_path, encoding=\"ISO-8859-1\", mode=\"r\")\n",
    "    lines = file.readlines()\n",
    "    pattern = r\"([^\\t]+)\\t([^\\t]+)\\t([^\\t]+)\\t([^\\t]+)\"\n",
    "    dictionary = dict()\n",
    "    for line in lines[1::]:\n",
    "        match = re.search(pattern, line)\n",
    "        word = match[1]\n",
    "        activeness = float(match[2])\n",
    "        evaluation = float(match[3])\n",
    "        imagery = float(match[4].strip())\n",
    "        \n",
    "        dictionary[word] = tuple([activeness, evaluation, imagery])\n",
    "    \n",
    "    return dictionary\n",
    "\n",
    "dal = load_dal('dict_of_affect.txt')\n",
    "\n",
    "def score_snippet(preprocessed_snippet, dal):\n",
    "    scores = np.empty((len(preprocessed_snippet),3))\n",
    "    index = 0\n",
    "    for token, pos in preprocessed_snippet:\n",
    "        if not re.search(\"EDIT_\", token):\n",
    "            match = re.search(r\"(NOT_)?(.+)\", token)\n",
    "            if match[2] in dal:\n",
    "                activeness, evaluation, imagery = dal[match[2]]\n",
    "                if match[1]:\n",
    "                    not_tag = -1\n",
    "                else:\n",
    "                    not_tag = 1\n",
    "\n",
    "                scores[index] = not_tag * np.asarray([activeness, evaluation, imagery])\n",
    "                index += 1\n",
    "                \n",
    "    if index == 0:\n",
    "        return np.zeros(3)\n",
    "    return np.average(scores[0:index], axis=0)\n",
    "    \n",
    "\n",
    "# The new get_features!\n",
    "def get_features(preprocessed_snippet):\n",
    "    feature_counts = np.zeros(dictionary_size + 3)\n",
    "    for (token, pos) in preprocessed_snippet:\n",
    "        if not re.search(\"EDIT\", token):\n",
    "            if token in dictionary:\n",
    "                feature_index = dictionary[token]\n",
    "                feature_counts[feature_index] = feature_counts[feature_index] + 1\n",
    "    scores = score_snippet(preprocessed_snippet, dal)\n",
    "    feature_counts[-3::] = scores\n",
    "    return feature_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tenkichi\\.julia\\conda\\3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression score (with DAL only):\n",
      "precision: 0.7243107769423559\n",
      "recall: 0.8731117824773413\n",
      "fmeasure: 0.7917808219178082\n",
      "[('<evaluation>', 4.644123390530019), ('too', -3.762356773945614), ('bad', -2.8696322890617796), ('dull', -2.4411542290778008), ('fails', -2.149108086754406), ('boring', -1.970887148822075), ('engrossing', 1.9673533610089007), ('funny', 1.955179795668551), ('entertaining', 1.9073668232404466), ('worth', 1.7638530239227905)]\n"
     ]
    }
   ],
   "source": [
    "X_train = np.empty((len(trn), dictionary_size + 3))\n",
    "Y_train = np.empty(len(trn)).astype(int)\n",
    "for i, (datapoint, label) in enumerate(trn):\n",
    "    Y_train[i] = label\n",
    "    features = get_features(datapoint)\n",
    "    X_train[i] = features\n",
    "\n",
    "    \n",
    "X_test = np.empty((len(tst), dictionary_size + 3))\n",
    "Y_true = np.empty(len(tst)).astype(int)\n",
    "for i, (datapoint, label) in enumerate(tst):\n",
    "    Y_true[i] = label\n",
    "    features = get_features(datapoint)\n",
    "    X_test[i] = features\n",
    "\n",
    "classifier_LR = LogisticRegression()\n",
    "model_new = classifier_LR.fit(normalize(X_train), Y_train)\n",
    "Y_pred = model_new.predict(normalize(X_test))\n",
    "precision, recall, fmeasure = evaluate_predictions(Y_pred, Y_true)\n",
    "print(\"Logistic Regression score (with DAL only):\")\n",
    "print(\"precision: {}\".format(precision))\n",
    "print(\"recall: {}\".format(recall))\n",
    "print(\"fmeasure: {}\".format(fmeasure))\n",
    "print(top_features(model_new, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the wn pos tag fom nltk pos tag\n",
    "def getWNTag(nltk_tag):\n",
    "    wntag = ''\n",
    "    if re.search(r\"^NN\", nltk_tag):\n",
    "        return wn.NOUN\n",
    "    if re.search(r\"^JJ\", nltk_tag):\n",
    "        return wn.ADJ\n",
    "    if re.search(r\"^VB\", nltk_tag):\n",
    "        return wn.VERB\n",
    "    if re.search(r\"^RB\", nltk_tag):\n",
    "        return wn.ADV\n",
    "    return \"\"\n",
    "\n",
    "# Need the lemmatizer to get roots of words to be used in synonym/antonym search\n",
    "wnl = WordNetLemmatizer()\n",
    "def score_snippet(preprocessed_snippet, dal):\n",
    "    scores = np.empty((len(preprocessed_snippet), 3))\n",
    "    index = 0\n",
    "    for token, pos in preprocessed_snippet:\n",
    "        # if edit tag is present, ignore\n",
    "        if not re.search(\"EDIT_\", token):\n",
    "            match = re.search(r\"(NOT_)?(.+)\", token)\n",
    "            final = \"\"\n",
    "            isAntonym = False\n",
    "            if match[2] in dal:\n",
    "                final = match[2]\n",
    "            else:\n",
    "                wntag = getWNTag(pos)\n",
    "                # we enter this case if the token word is not in dal, so go through the synonyms/antonyms\n",
    "                if wntag is not \"\":\n",
    "                    # Basically, everytime something goes wrong, it throws errors\n",
    "                    # If it can't lemmateize - error\n",
    "                    # If it can't find a proper synonym, antonym within wn routings, error\n",
    "                    # Since more often than not if it errors out in one plase, it errors out everywhere else\n",
    "                    # I've opted to have a try-except statement the whole section here\n",
    "                    try:\n",
    "                        lemma = wnl.lemmatize(match[2], pos=wntag)\n",
    "                        word = lemma + \".\" + wntag + \".01\" # always use 1st definition since I don't know how to find correct one\n",
    "                        word = wn.synset(word)\n",
    "                        found = False\n",
    "                        # First we go through hypernyms\n",
    "                        for hypernym in word.hypernyms():\n",
    "                            synonym = hypernym.name()\n",
    "                            if synonym in dal:\n",
    "                                final = synonym\n",
    "                                break\n",
    "                        # Then we go through hyponyms\n",
    "                        if not found:\n",
    "                            for hyponym in word.hyponyms():\n",
    "                                synonym = hyponym.name()\n",
    "                                if synonym in dal:\n",
    "                                    final = synonym\n",
    "                                    break\n",
    "                        # Then the possible holonyms\n",
    "                        if not found:\n",
    "                            for holonym in word.member_holonyms():\n",
    "                                synonym = holonym.name()\n",
    "                                if synonym in dal:\n",
    "                                    final = synonym\n",
    "                                    break\n",
    "                        # Then finally the antonyms\n",
    "                        if not found:\n",
    "                            for antonym in word.lemmas()[0].antonyms():\n",
    "                                if antonym.name() in dal:\n",
    "                                    final = antonym.name()\n",
    "                                    isAntonym = True\n",
    "                    except:\n",
    "                        final = \"\"\n",
    "                else:\n",
    "                    final = \"\"\n",
    "            \n",
    "            if final is not \"\":\n",
    "                activeness, evaluation, imagery = dal[final]\n",
    "                if match[1]:\n",
    "                    not_tag = -1\n",
    "                else:\n",
    "                    not_tag = 1\n",
    "                    \n",
    "                if isAntonym:\n",
    "                    not_tag *= -1\n",
    "\n",
    "                scores[index] = not_tag * np.asarray([activeness, evaluation, imagery])\n",
    "                index += 1\n",
    "            # of course if everything fails, then we skip the word by not appending anything\n",
    "    # If there are no words, we just return [0,0,0]\n",
    "    if index == 0:\n",
    "        return np.zeros(3)\n",
    "    return np.average(scores[0: index], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tenkichi\\.julia\\conda\\3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Score (with DAL and WebNet):\n",
      "precision: 0.7425474254742548\n",
      "recall: 0.8277945619335347\n",
      "fmeasure: 0.7828571428571428\n",
      "[('<evaluation>', 4.643010494798919), ('too', -3.760428448374615), ('bad', -2.8750986550254027), ('dull', -2.4488438484317268), ('fails', -2.1588851186760625), ('funny', 1.9690625524063698), ('engrossing', 1.9600344824502434), ('boring', -1.9585515007958705), ('entertaining', 1.9131901405153682), ('worth', 1.7676686456282218)]\n"
     ]
    }
   ],
   "source": [
    "X_train = np.empty((len(trn), dictionary_size + 3))\n",
    "Y_train = np.empty(len(trn)).astype(int)\n",
    "for i, (datapoint, label) in enumerate(trn):\n",
    "    Y_train[i] = label\n",
    "    features = get_features(datapoint)\n",
    "    X_train[i] = features\n",
    "\n",
    "    \n",
    "X_test = np.empty((len(tst), dictionary_size + 3))\n",
    "Y_true = np.empty(len(tst)).astype(int)\n",
    "for i, (datapoint, label) in enumerate(tst):\n",
    "    Y_true[i] = label\n",
    "    features = get_features(datapoint)\n",
    "    X_test[i] = features\n",
    "\n",
    "classifier_LR = LogisticRegression()\n",
    "model_new = classifier_LR.fit(normalize(X_train), Y_train)\n",
    "Y_pred = model_new.predict(normalize(X_test))\n",
    "precision, recall, fmeasure = evaluate_predictions(Y_pred, Y_true)\n",
    "print(\"Logistic Regression Score (with DAL and WebNet):\")\n",
    "print(\"precision: {}\".format(precision))\n",
    "print(\"recall: {}\".format(recall))\n",
    "print(\"fmeasure: {}\".format(fmeasure))\n",
    "print(top_features(model_new, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
